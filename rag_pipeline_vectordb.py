# -*- coding: utf-8 -*-
"""rag_pipeline/VectorDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15rdQnrETP6ICpRNU6AtFLsGbgw2-4UBv
"""

# Step 1: Install langchain core (base package)
!pip install langchain

# Step 2: Install langchain-community for document loaders
!pip install langchain-community

# Step 3: Install chromadb for vector database
!pip install chromadb

# Step 4: Install Google Gemini integration
!pip install langchain-google-genai

# Step 5: Install HuggingFace for embeddings
!pip install langchain-huggingface

# Step 6: Install sentence transformers (required for embeddings)
!pip install sentence-transformers

# Step 7: Install langchain-chroma (ChromaDB integration)
!pip install langchain-chroma

# Step 8: Install Google GenAI SDK
!pip install google-generativeai

import os
GOOGLE_API_KEY = "#"  # ⚠️ CHANGE THIS!

os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

from langchain_community.document_loaders import DirectoryLoader, TextLoader


import os
import zipfile

DATA_PATH = "./data/Interview_questions"  # Update this path

# Define the path to the zip file and the target directory
ZIP_FILE_PATH = "/content/Interview_questions.zip"
TARGET_DIR = "./data"

# Create the target directory if it doesn't exist
os.makedirs(TARGET_DIR, exist_ok=True)

# Unzip the file if it exists
if os.path.exists(ZIP_FILE_PATH):
    with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:
        zip_ref.extractall(TARGET_DIR)
    print(f"✅ Unzipped '{ZIP_FILE_PATH}' to '{TARGET_DIR}'")
else:
    print(f"⚠️ Zip file not found: {ZIP_FILE_PATH}. Please ensure it's uploaded or check the path.")

print(f"Loading documents from: {DATA_PATH}")

# Load Data
loader = DirectoryLoader(
    DATA_PATH,
    glob="**/*.txt",
    loader_cls=TextLoader
)

documents = loader.load()

print(f" Loaded {len(documents)} documents")

# Text Splitting
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)

texts = text_splitter.split_documents(documents)

print(f" Created {len(texts)} text chunks")

# View first chunk (optional)
print("\nFirst chunk preview:")
print(texts[0].page_content[:200] + "...")
len(texts)
print("___________")
texts[0]

# Embeddings (FREE - HuggingFace)
from langchain_huggingface import HuggingFaceEmbeddings



# For Colab with GPU, use 'cuda'
# For CPU, use 'cpu'
embedding = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={'device': 'cuda'},  # Change to 'cpu' if no GPU
    encode_kwargs={'normalize_embeddings': True}
)

# Vector Database (ChromaDB)
from langchain_chroma import Chroma

vectordb = Chroma.from_documents(
    documents=texts,
    embedding=embedding,
    persist_directory="./chroma_db"
)

print(f"✅ Vector database created with {len(texts)} vectors")

retriever=vectordb.as_retriever()
docs=retriever.invoke("Tell me about Numphy")
len(docs)

docs

retriever=vectordb.as_retriever(search_kwargs={"k":2})

retriever

retriever.search_type

# LLM (FREE - Google Gemini)
from langchain_google_genai import ChatGoogleGenerativeAI

# Chain
from langchain_classic.chains import RetrievalQA


llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    convert_system_message_to_human=True
)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)
qa_chain("what is python")

def process_llm_response(llm_response):
    print(llm_response['result'])
    print('\n\nSources:')
    for source in llm_response["source_documents"]:
        print(source.metadata['source'])

prompt = "What is Python"
llm_response = qa_chain(prompt)
process_llm_response(llm_response)

import os

current_api_key = os.environ.get("GOOGLE_API_KEY", "Not Set")
if current_api_key != "Not Set" and len(current_api_key) > 8:
    print(f"Current Google API Key (masked): {current_api_key[:4]}...{current_api_key[-4:]}")
else:
    print("Google API Key is not set or is too short.")
print("Please ensure this key is valid and has access to Gemini models. If you need to update it, please modify cell qH-ZmiUSlWWG with your correct key.")

import google.generativeai as genai
import os

# Ensure the API key is configured. It should have been set in qH-ZmiUSlWWG
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
    print("✅ Google Generative AI configured.")
else:
    print("⚠️ GOOGLE_API_KEY is not set. Please set it in cell qH-ZmiUSlWWG and re-run.")


print("\nAvailable Gemini models (supporting generateContent):")
for m in genai.list_models():
    if "generateContent" in m.supported_generation_methods and "gemini" in m.name:
        print(m.name)